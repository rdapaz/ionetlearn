{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax Normalization\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{Z}\\tiny{i} & = \\frac{x\\tiny{i} \\small- min(X)}{max(X) - min(X)} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu function\n",
    "\n",
    "The ReLU function helps wiht the problem with vanishing gradients in deep networks by not squashing both ends\n",
    "\n",
    "\\begin{align}\n",
    "ReLU(x) = max(x, 0)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "The sigmoid function generates a smooth non-linear curve that squashes incoming values between 0 and 1.  It usually works well with classifiers that have had the input values scaled but can be problematic for high input values in that the information in these values will be lost.  This function is often used along with the loss function binary cross entropy.\n",
    "\n",
    "\\begin{align}\n",
    "f(x) & = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "The softmax function accepts and array of values and returns an array of values.  It highlights the largest values and suppresses values that are below the maximum value by a significant margin.  It normalises the outputs so that they add up to 1 and can be directly treated as probabilities over the output.  For this reason, it is ideal for use in multi-class classifier models.\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x\\tiny{j}\\normalsize{)} & = e^{x\\tiny{j}}*(\\sum_{k=1}^{K}e^{x\\tiny{k}})^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross entropy\n",
    "\n",
    "The following is the formula for `binary cross entropy`.  The function measures how far from the true value (i.e., either 0 or 1), the prediction is, averaging these class-wise errors to calculate the final loss.\n",
    "\n",
    "\\begin{align}\n",
    "L(y,\\hat{y}) & = -\\frac{1}{N} \\sum_{n=0}^{N} (\\normalsize{{y*log(\\hat{y}}\\tiny{i}\\normalsize){ + (1 - y)*log(1-\\hat{y}}\\tiny{i}\\normalsize))}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $\\hat{y}$ is the predicted value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical cross entropy\n",
    "\n",
    "Categorical crossentropy will compare the distribution of predictions (the activations in the output layer, one for each class) wiht the true distribution, where the probability of the true class is set to 1 and 0 for the other classes.  The true class is represented as a one hot encoded vector and the closer the model's outputs are to the vector, the lower the loss.\n",
    "\n",
    "\\begin{align}\n",
    "L(y,\\hat{y}) & = - \\sum_{j=0}^{M} \\sum_{i=0}^{N}\\normalsize{(y}\\tiny{ij} \\normalsize{ * log(\\hat{y}}\\tiny{ij} \\normalsize{ ))}\n",
    "\\end{align}\n",
    "where $\\hat{y}$ is the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
